{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf88bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutikmk/Documents/90DAYS/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fitz  # pymupdf\n",
    "import arxiv\n",
    "\n",
    "# --- Embeddings for Chonkie's semantic chunker ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Try to import chonkie (required for semantic chunking here)\n",
    "import importlib\n",
    "try:\n",
    "    chonkie = importlib.import_module(\"chonkie\")\n",
    "    _HAS_CHONKIE = True\n",
    "except Exception:\n",
    "    _HAS_CHONKIE = False\n",
    "    raise ImportError(\"Chonkie is required for the semantic chunker. Please install it: `pip install chonkie`\")\n",
    "\n",
    "DATA_DIR = Path('data/raw/arxiv')\n",
    "PDF_DIR = DATA_DIR / 'pdfs'\n",
    "TEXT_DIR = DATA_DIR / 'texts'\n",
    "for d in [DATA_DIR, PDF_DIR, TEXT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ARXIV_REQUEST_DELAY_S = 3.0\n",
    "HEADERS = {'User-Agent': 'Arxiv-RAG-Preproc/0.1 (contact: shrutikmkulkarni.work@gmail.com)'}  # <-- put your email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdad250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url: str, out_path: Path, headers: Optional[dict] = None, retry: int = 3) -> Path:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            with requests.get(pdf_url, stream=True, headers=headers, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(out_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return out_path\n",
    "        except Exception:\n",
    "            if attempt == retry - 1:\n",
    "                raise\n",
    "            time.sleep(2 + attempt)\n",
    "    return out_path\n",
    "\n",
    "def extract_pdf_text(pdf_path: Path) -> Tuple[str, List[Tuple[int, str]]]:\n",
    "    \"\"\"Return full_text and per-page list[(page_index, text)].\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for i, page in enumerate(doc):\n",
    "        txt = page.get_text(\"text\")\n",
    "        pages.append((i, txt))\n",
    "    full_text = \"\\n\".join(t for _, t in pages)\n",
    "    return full_text, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea16aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hugging Face auth (optional) ---\n",
    "import os\n",
    "HF_TOKEN = os.getenv(\"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\")  # set this in your env if needed\n",
    "\n",
    "# ---------- Chonkie Semantic Chunker adapter ----------\n",
    "# Use all-MiniLM-L6-v2 by name; pass only kwargs supported by SentenceTransformer\n",
    "def _semantic_chunk_with_known_apis(full_text: str) -> list[dict]:\n",
    "    import importlib\n",
    "    chonkie = importlib.import_module(\"chonkie\")\n",
    "\n",
    "    # Kwargs that SentenceTransformer actually understands\n",
    "    st_kwargs = {}\n",
    "    if HF_TOKEN:\n",
    "        st_kwargs[\"use_auth_token\"] = HF_TOKEN\n",
    "\n",
    "    # Prefer giving the model name string. Chonkie will instantiate it.\n",
    "    # You may also add device=\"mps\" on Apple Silicon, or \"cpu\"/\"cuda\" elsewhere.\n",
    "    sc = None\n",
    "    if hasattr(chonkie, \"SemanticChunker\"):\n",
    "        sc = chonkie.SemanticChunker(\n",
    "            embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            # device=\"mps\",        # uncomment for Apple Silicon\n",
    "            **st_kwargs\n",
    "        )\n",
    "    elif hasattr(chonkie, \"semantic\") and hasattr(chonkie.semantic, \"SemanticChunker\"):\n",
    "        sc = chonkie.semantic.SemanticChunker(\n",
    "            embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            # device=\"mps\",\n",
    "            **st_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ImportError(\"Could not locate Chonkie's SemanticChunker in this version.\")\n",
    "\n",
    "    # Try the common methods in order\n",
    "    if hasattr(sc, \"chunk_text\") and callable(sc.chunk_text):\n",
    "        pieces = sc.chunk_text(full_text)\n",
    "    elif hasattr(sc, \"split\") and callable(sc.split):\n",
    "        pieces = sc.split(full_text)\n",
    "    elif hasattr(sc, \"chunk\") and callable(sc.chunk):\n",
    "        pieces = sc.chunk(full_text)\n",
    "    else:\n",
    "        raise AttributeError(\"SemanticChunker found, but no method chunk_text/split/chunk detected.\")\n",
    "\n",
    "    out = []\n",
    "    for i, p in enumerate(pieces):\n",
    "        if isinstance(p, dict):\n",
    "            text = p.get(\"text\") or p.get(\"chunk\") or \"\"\n",
    "            title = p.get(\"title\") or p.get(\"section_title\") or f\"Chunk {i}\"\n",
    "            sp = p.get(\"start_page\")\n",
    "            ep = p.get(\"end_page\")\n",
    "        else:\n",
    "            text = getattr(p, \"text\", str(p))\n",
    "            title = getattr(p, \"title\", f\"Chunk {i}\")\n",
    "            sp = getattr(p, \"start_page\", None)\n",
    "            ep = getattr(p, \"end_page\", None)\n",
    "        out.append(\n",
    "            {\"section_title\": title, \"text\": text, \"start_page\": sp, \"end_page\": ep}\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def chunk_document_semantic(full_text: str) -> list[dict]:\n",
    "    return _semantic_chunk_with_known_apis(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2dc174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " ['http://arxiv.org/abs/2508.13152v1',\n",
       "  'http://arxiv.org/abs/2508.13144v1',\n",
       "  'http://arxiv.org/abs/2508.13142v1',\n",
       "  'http://arxiv.org/abs/2508.13141v1'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_four_arxiv_papers(query: str = 'cat:cs.CL', max_results: int = 4) -> List[arxiv.Result]:\n",
    "    client = arxiv.Client(page_size=25, delay_seconds=ARXIV_REQUEST_DELAY_S, num_retries=3)\n",
    "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "    results = list(client.results(search))\n",
    "    return results\n",
    "\n",
    "results = fetch_four_arxiv_papers()\n",
    "len(results), [r.entry_id for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae92ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDFs: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>entry_id</th>\n",
       "      <th>source</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>RepreGuard: Detecting LLM-Generated Text by Re...</td>\n",
       "      <td>Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Z...</td>\n",
       "      <td>Detecting content generated by large language ...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL, cs.AI</td>\n",
       "      <td>2025-08-18 17:59:15+00:00</td>\n",
       "      <td>2025-08-18 17:59:15+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2508.13152v1</td>\n",
       "      <td>http://arxiv.org/abs/2508.13152v1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>RepreGuard: Detecting LLM-Generated Text by Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2508.13144v1</td>\n",
       "      <td>Signal and Noise: A Framework for Reducing Unc...</td>\n",
       "      <td>David Heineman, Valentin Hofmann, Ian Magnusso...</td>\n",
       "      <td>Developing large language models is expensive ...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL, cs.LG</td>\n",
       "      <td>2025-08-18 17:56:04+00:00</td>\n",
       "      <td>2025-08-18 17:56:04+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2508.13144v1</td>\n",
       "      <td>http://arxiv.org/abs/2508.13144v1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>Signal and Noise: A Framework for Reducing\\nUn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508.13142v1</td>\n",
       "      <td>Has GPT-5 Achieved Spatial Intelligence? An Em...</td>\n",
       "      <td>Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi W...</td>\n",
       "      <td>Multi-modal models have achieved remarkable pr...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV, cs.CL, cs.LG, cs.MM, cs.RO</td>\n",
       "      <td>2025-08-18 17:55:17+00:00</td>\n",
       "      <td>2025-08-18 17:55:17+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2508.13142v1</td>\n",
       "      <td>http://arxiv.org/abs/2508.13142v1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>Has GPT-5 Achieved Spatial Intelligence?\\nAn E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2508.13141v1</td>\n",
       "      <td>OptimalThinkingBench: Evaluating Over and Unde...</td>\n",
       "      <td>Pranjal Aggarwal, Seungone Kim, Jack Lanchanti...</td>\n",
       "      <td>Thinking LLMs solve complex tasks at the expen...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL, cs.LG</td>\n",
       "      <td>2025-08-18 17:53:10+00:00</td>\n",
       "      <td>2025-08-18 17:53:10+00:00</td>\n",
       "      <td>http://arxiv.org/pdf/2508.13141v1</td>\n",
       "      <td>http://arxiv.org/abs/2508.13141v1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>OptimalThinkingBench: Evaluating Over and\\nUnd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_id                                              title  \\\n",
       "0  2508.13152v1  RepreGuard: Detecting LLM-Generated Text by Re...   \n",
       "1  2508.13144v1  Signal and Noise: A Framework for Reducing Unc...   \n",
       "2  2508.13142v1  Has GPT-5 Achieved Spatial Intelligence? An Em...   \n",
       "3  2508.13141v1  OptimalThinkingBench: Evaluating Over and Unde...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Z...   \n",
       "1  David Heineman, Valentin Hofmann, Ian Magnusso...   \n",
       "2  Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi W...   \n",
       "3  Pranjal Aggarwal, Seungone Kim, Jack Lanchanti...   \n",
       "\n",
       "                                            abstract primary_category  \\\n",
       "0  Detecting content generated by large language ...            cs.CL   \n",
       "1  Developing large language models is expensive ...            cs.CL   \n",
       "2  Multi-modal models have achieved remarkable pr...            cs.CV   \n",
       "3  Thinking LLMs solve complex tasks at the expen...            cs.CL   \n",
       "\n",
       "                          categories                 published  \\\n",
       "0                       cs.CL, cs.AI 2025-08-18 17:59:15+00:00   \n",
       "1                       cs.CL, cs.LG 2025-08-18 17:56:04+00:00   \n",
       "2  cs.CV, cs.CL, cs.LG, cs.MM, cs.RO 2025-08-18 17:55:17+00:00   \n",
       "3                       cs.CL, cs.LG 2025-08-18 17:53:10+00:00   \n",
       "\n",
       "                    updated                            pdf_url  \\\n",
       "0 2025-08-18 17:59:15+00:00  http://arxiv.org/pdf/2508.13152v1   \n",
       "1 2025-08-18 17:56:04+00:00  http://arxiv.org/pdf/2508.13144v1   \n",
       "2 2025-08-18 17:55:17+00:00  http://arxiv.org/pdf/2508.13142v1   \n",
       "3 2025-08-18 17:53:10+00:00  http://arxiv.org/pdf/2508.13141v1   \n",
       "\n",
       "                            entry_id source  \\\n",
       "0  http://arxiv.org/abs/2508.13152v1  arxiv   \n",
       "1  http://arxiv.org/abs/2508.13144v1  arxiv   \n",
       "2  http://arxiv.org/abs/2508.13142v1  arxiv   \n",
       "3  http://arxiv.org/abs/2508.13141v1  arxiv   \n",
       "\n",
       "                                            raw_text  \n",
       "0  RepreGuard: Detecting LLM-Generated Text by Re...  \n",
       "1  Signal and Noise: A Framework for Reducing\\nUn...  \n",
       "2  Has GPT-5 Achieved Spatial Intelligence?\\nAn E...  \n",
       "3  OptimalThinkingBench: Evaluating Over and\\nUnd...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_rows = []\n",
    "for r in tqdm(results, desc='Downloading PDFs'):\n",
    "    paper_id = r.get_short_id()\n",
    "    pdf_path = PDF_DIR / f\"{paper_id}.pdf\"\n",
    "    if not pdf_path.exists():\n",
    "        download_pdf(r.pdf_url, pdf_path, headers=HEADERS)\n",
    "        time.sleep(ARXIV_REQUEST_DELAY_S)\n",
    "\n",
    "    full_text, _pages = extract_pdf_text(pdf_path)\n",
    "    (TEXT_DIR / f\"{paper_id}.txt\").write_text(full_text, encoding='utf-8')\n",
    "\n",
    "    meta = {\n",
    "        'paper_id': paper_id,\n",
    "        'title': r.title,\n",
    "        'authors': \", \".join(a.name for a in r.authors),\n",
    "        'abstract': r.summary,\n",
    "        'primary_category': r.primary_category,\n",
    "        'categories': \", \".join(r.categories),\n",
    "        'published': r.published,\n",
    "        'updated': r.updated,\n",
    "        'pdf_url': r.pdf_url,\n",
    "        'entry_id': r.entry_id,\n",
    "        'source': 'arxiv',\n",
    "        'raw_text': full_text,\n",
    "    }\n",
    "    paper_rows.append(meta)\n",
    "\n",
    "papers_df = pd.DataFrame(paper_rows)\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58808fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic chunking: 100%|██████████| 4/4 [00:52<00:00, 13.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>section_title</th>\n",
       "      <th>text</th>\n",
       "      <th>start_page</th>\n",
       "      <th>end_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>2508.13152v1::chunk_0000</td>\n",
       "      <td>Chunk 0</td>\n",
       "      <td>RepreGuard: Detecting LLM-Generated Text by Re...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>2508.13152v1::chunk_0001</td>\n",
       "      <td>Chunk 1</td>\n",
       "      <td>Junchao Wu1,∗\\nShu Yang3\\nRunzhe Zhan1\\nZeyu W...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>2508.13152v1::chunk_0002</td>\n",
       "      <td>Chunk 2</td>\n",
       "      <td>1NLP2CT Lab, Department of Computer and Inform...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>2508.13152v1::chunk_0003</td>\n",
       "      <td>Chunk 3</td>\n",
       "      <td>min.yang@siat.ac.cn, cszyluo@comp.hkbu.edu.hk,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2508.13152v1</td>\n",
       "      <td>2508.13152v1::chunk_0004</td>\n",
       "      <td>Chunk 4</td>\n",
       "      <td>Abstract\\nDetecting content generated by large...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_id                  chunk_id section_title  \\\n",
       "0  2508.13152v1  2508.13152v1::chunk_0000       Chunk 0   \n",
       "1  2508.13152v1  2508.13152v1::chunk_0001       Chunk 1   \n",
       "2  2508.13152v1  2508.13152v1::chunk_0002       Chunk 2   \n",
       "3  2508.13152v1  2508.13152v1::chunk_0003       Chunk 3   \n",
       "4  2508.13152v1  2508.13152v1::chunk_0004       Chunk 4   \n",
       "\n",
       "                                                text start_page end_page  \n",
       "0  RepreGuard: Detecting LLM-Generated Text by Re...       None     None  \n",
       "1  Junchao Wu1,∗\\nShu Yang3\\nRunzhe Zhan1\\nZeyu W...       None     None  \n",
       "2  1NLP2CT Lab, Department of Computer and Inform...       None     None  \n",
       "3  min.yang@siat.ac.cn, cszyluo@comp.hkbu.edu.hk,...       None     None  \n",
       "4  Abstract\\nDetecting content generated by large...       None     None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_rows = []\n",
    "for _, row in tqdm(papers_df.iterrows(), total=len(papers_df), desc='Semantic chunking'):\n",
    "    full_text = row.raw_text\n",
    "    chunks = chunk_document_semantic(full_text)\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_rows.append({\n",
    "            'paper_id': row.paper_id,\n",
    "            'chunk_id': f\"{row.paper_id}::chunk_{idx:04d}\",\n",
    "            'section_title': ch.get('section_title'),\n",
    "            'text': ch.get('text'),\n",
    "            'start_page': ch.get('start_page'),\n",
    "            'end_page': ch.get('end_page'),\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(chunk_rows)\n",
    "chunks_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
